{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\nimport pandas as pd\nimport numpy as np\n\ndataset = load_dataset(\"openlifescienceai/medmcqa\")\ndataset = dataset['train'].to_pandas()[\n    [\"question\", \"opa\", \"opb\",\"opc\",\"opd\",\"cop\",\"exp\"]\n]\n\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:45:20.894051Z","iopub.execute_input":"2025-09-24T15:45:20.894331Z","iopub.status.idle":"2025-09-24T15:45:28.663861Z","shell.execute_reply.started":"2025-09-24T15:45:20.894309Z","shell.execute_reply":"2025-09-24T15:45:28.663145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Converting each dataset entry into a prompt-response format suitable for the LLM.\ndef convert_row(row):\n    prompt = f\"\"\"Question: {row['question']}\nOptions:\na(0) {row['opa']}\nb(1) {row['opb']}\nc(2) {row['opc']}\nd(3) {row['opd']}\nChoose the correct answer and explain why.\"\"\"\n    \n    response = f\"<answer>{row['cop']}</answer>\\n<think>{row['exp']}</think>\"\n    return {\"prompt\": prompt, \"response\": response}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:45:28.665241Z","iopub.execute_input":"2025-09-24T15:45:28.665649Z","iopub.status.idle":"2025-09-24T15:45:28.669788Z","shell.execute_reply.started":"2025-09-24T15:45:28.665629Z","shell.execute_reply":"2025-09-24T15:45:28.669099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"converted = dataset.apply(convert_row, axis=1, result_type=\"expand\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:45:28.670632Z","iopub.execute_input":"2025-09-24T15:45:28.670909Z","iopub.status.idle":"2025-09-24T15:45:34.909588Z","shell.execute_reply.started":"2025-09-24T15:45:28.670882Z","shell.execute_reply":"2025-09-24T15:45:34.908902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\"  # Enables extra vLLM kernels\n# Remove existing PyTorch to avoid conflicts\n!pip install -qqq pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install -qqq torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install -qqq unsloth\n!pip install -qqq vllm==0.9.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:45:34.911499Z","iopub.execute_input":"2025-09-24T15:45:34.911744Z","iopub.status.idle":"2025-09-24T15:52:11.007862Z","shell.execute_reply.started":"2025-09-24T15:45:34.911726Z","shell.execute_reply":"2025-09-24T15:52:11.006676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import unsloth\nfrom unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 512 # Can increase for longer reasoning traces\nlora_rank = 32 # Larger rank = smarter, but slower\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Qwen3-4B-Base\",\n    max_seq_length = max_seq_length,\n    load_in_4bit = False, # False for LoRA 16bit\n    fast_inference = False, # Enable vLLM fast inference\n    max_lora_rank = lora_rank,\n    gpu_memory_utilization = 0.9, # Reduce if out of memory\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n    lora_alpha = lora_rank*2, # *2 speeds up training\n    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n    random_state = 3407,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:52:11.009173Z","iopub.execute_input":"2025-09-24T15:52:11.009645Z","iopub.status.idle":"2025-09-24T15:54:32.167627Z","shell.execute_reply.started":"2025-09-24T15:52:11.009604Z","shell.execute_reply":"2025-09-24T15:54:32.166535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title System Prompt { display-mode: \"form\" }\n# Chat Template for GRPO\nexplanation_start = \"<think>\" # Acts as <think>\nexplanation_end   = \"</think>\"   # Acts as </think>\nsolution_start  = \"<answer>\"\nsolution_end    = \"</answer>\"\n\nsystem_prompt = \\\nf\"\"\"You are given a question from a Medical Entrance Exam.  \nEach question has four options: \"a(0)\", \"b(1)\", \"c(2)\", and \"d(3)\".  \nSome questions may have multiple correct answers.  \nFirst, provide the correct answer(s) enclosed between {solution_start} and {solution_end}\nThen, think through the question carefully and explain your reasoning.  \nEnclose your reasoning between {explanation_start} and {explanation_end}.\"\"\"\nsystem_prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:54:32.170332Z","iopub.execute_input":"2025-09-24T15:54:32.170958Z","iopub.status.idle":"2025-09-24T15:54:32.176991Z","shell.execute_reply.started":"2025-09-24T15:54:32.170929Z","shell.execute_reply":"2025-09-24T15:54:32.176185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Chat Template { display-mode: \"form\" }\nchat_template = \\\n    \"{% if messages[0]['role'] == 'system' %}\"\\\n        \"{{ messages[0]['content'] + eos_token }}\"\\\n        \"{% set loop_messages = messages[1:] %}\"\\\n    \"{% else %}\"\\\n        \"{{ '{system_prompt}' + eos_token }}\"\\\n        \"{% set loop_messages = messages %}\"\\\n    \"{% endif %}\"\\\n    \"{% for message in loop_messages %}\"\\\n        \"{% if message['role'] == 'user' %}\"\\\n            \"{{ message['content'] }}\"\\\n        \"{% elif message['role'] == 'assistant' %}\"\\\n            \"{{ message['content'] + eos_token }}\"\\\n        \"{% endif %}\"\\\n    \"{% endfor %}\"\\\n    \"{% if add_generation_prompt %}{{ '{solution_start}' }}\"\\\n    \"{% endif %}\"\n\n# Replace with out specific template:\nchat_template = chat_template\\\n    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n    .replace(\"'{reasoning_start}'\", f\"'{solution_start}'\")  \ntokenizer.chat_template = chat_template","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:54:32.178070Z","iopub.execute_input":"2025-09-24T15:54:32.178401Z","iopub.status.idle":"2025-09-24T15:54:34.450699Z","shell.execute_reply.started":"2025-09-24T15:54:32.178372Z","shell.execute_reply":"2025-09-24T15:54:34.450049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title GRPO Template { display-mode: \"form\" }\ndef format_dataset(x):\n    # The correct_options has numeric values [0,1,2,3], while the options have alphabets [a,b,c,d]\n    # We map the intergers to the alphabets\n    idx2letter = {0:\"a\", 1:\"b\", 2:\"c\", 3:\"d\"}\n    correct_answer = idx2letter[x[\"cop\"]]\n\n    # The user prompt is the question + options\n    question = converted.iloc[x.name][\"prompt\"]\n\n    # Get the Explanations. Some of the questions also have no explanation to them\n    explanation = x[\"exp\"]\n    if explanation is None or str(explanation).lower() in [\"none\", \"nan\", \"null\", \"\"]:\n        explanation = \"\"\n    explanation = str(explanation).strip()\n\n\n    # Add our custom formatting\n    final_prompt = \\\n        solution_start + correct_answer  + solution_end + \\\n        explanation_start + explanation + explanation_end \n\n    return [\n        {\"role\" : \"system\",    \"content\" : system_prompt},\n        {\"role\" : \"user\",      \"content\" : question},\n        {\"role\" : \"assistant\", \"content\" : final_prompt},\n    ]\n\ndataset[\"Messages\"] = dataset.apply(format_dataset, axis = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:54:34.451455Z","iopub.execute_input":"2025-09-24T15:54:34.451654Z","iopub.status.idle":"2025-09-24T15:54:40.035783Z","shell.execute_reply.started":"2025-09-24T15:54:34.451638Z","shell.execute_reply":"2025-09-24T15:54:40.035204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.apply_chat_template(dataset[\"Messages\"][1], tokenize = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:54:40.036443Z","iopub.execute_input":"2025-09-24T15:54:40.036629Z","iopub.status.idle":"2025-09-24T15:54:40.046211Z","shell.execute_reply.started":"2025-09-24T15:54:40.036614Z","shell.execute_reply":"2025-09-24T15:54:40.045496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\ndataset[\"text\"] = tokenizer.apply_chat_template(dataset[\"Messages\"].values.tolist(), tokenize = False)\ndataset = Dataset.from_pandas(dataset)\n# Dataset in required prompt respomse format\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:54:40.049542Z","iopub.execute_input":"2025-09-24T15:54:40.049983Z","iopub.status.idle":"2025-09-24T15:54:47.856461Z","shell.execute_reply.started":"2025-09-24T15:54:40.049966Z","shell.execute_reply":"2025-09-24T15:54:47.855767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# To get the model used to the prompt style, we will only train it on 5000 questions\nsubset_size = 5000  # pick a few thousand\ndataset_small = dataset.shuffle(seed=42).select(range(subset_size))\n\nprint(f\"Using {len(dataset_small)} examples for pre-finetune\")\ndataset_small","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:54:47.857127Z","iopub.execute_input":"2025-09-24T15:54:47.857398Z","iopub.status.idle":"2025-09-24T15:54:47.914430Z","shell.execute_reply.started":"2025-09-24T15:54:47.857374Z","shell.execute_reply":"2025-09-24T15:54:47.913593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Fine Tune Model { display-mode: \"form\" }\nfrom trl import SFTTrainer, SFTConfig\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset_small,\n    args = SFTConfig(\n        dataset_text_field = \"text\",\n        per_device_train_batch_size = 1,\n        gradient_accumulation_steps = 1, # Use GA to mimic batch size!\n        warmup_steps = 5,\n        num_train_epochs = 2, # Set this for 1 full training run.\n        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n        logging_steps = 5,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:54:47.915309Z","iopub.execute_input":"2025-09-24T15:54:47.915619Z","iopub.status.idle":"2025-09-24T15:55:38.589463Z","shell.execute_reply.started":"2025-09-24T15:54:47.915601Z","shell.execute_reply":"2025-09-24T15:55:38.588761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T15:55:38.590438Z","iopub.execute_input":"2025-09-24T15:55:38.590733Z","iopub.status.idle":"2025-09-24T17:44:31.494766Z","shell.execute_reply.started":"2025-09-24T15:55:38.590707Z","shell.execute_reply":"2025-09-24T17:44:31.487716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Take just system + user, not the assistant answer\ntext = tokenizer.apply_chat_template(\n    dataset[48][\"Messages\"][:2],  # system + user only\n    tokenize=False,\n    add_generation_prompt=True   # so model knows it's assistant's turn\n)\n\nprint(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T17:47:07.896638Z","iopub.execute_input":"2025-09-24T17:47:07.896925Z","iopub.status.idle":"2025-09-24T17:47:07.902921Z","shell.execute_reply.started":"2025-09-24T17:47:07.896906Z","shell.execute_reply":"2025-09-24T17:47:07.902173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"_ = model.generate(\n    **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n    temperature=0,\n    max_new_tokens=128,\n    streamer=TextStreamer(tokenizer, skip_prompt=False),  # ðŸ‘ˆ skip prompt when printing\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T17:47:14.025878Z","iopub.execute_input":"2025-09-24T17:47:14.026399Z","iopub.status.idle":"2025-09-24T17:47:22.338009Z","shell.execute_reply.started":"2025-09-24T17:47:14.026377Z","shell.execute_reply":"2025-09-24T17:47:22.337322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_dir = \"./pre_finetuned_model_4B\"\n\ntrainer.save_model(save_dir)   # saves model + adapter weights (if any)\ntokenizer.save_pretrained(save_dir)  # saves tokenizer files too","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T17:48:26.329723Z","iopub.execute_input":"2025-09-24T17:48:26.330601Z","iopub.status.idle":"2025-09-24T17:48:27.463058Z","shell.execute_reply.started":"2025-09-24T17:48:26.330576Z","shell.execute_reply":"2025-09-24T17:48:27.462288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fine Tuned Model for Prompt Response Style\n!tar -czvf pre_finetuned_model_4B.tar.gz pre_finetuned_model_4B","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T17:48:50.354478Z","iopub.execute_input":"2025-09-24T17:48:50.354790Z","iopub.status.idle":"2025-09-24T17:49:05.927794Z","shell.execute_reply.started":"2025-09-24T17:48:50.354770Z","shell.execute_reply":"2025-09-24T17:49:05.926841Z"}},"outputs":[],"execution_count":null}]}