{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":591527,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":442531,"modelId":459074}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n!pip install --upgrade -qqq uv\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    # If you're not in Colab, just use pip install!\n    !pip install unsloth vllm\nelse:\n    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n    except: get_numpy = \"numpy\"\n    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n    except: is_t4 = False\n    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n    !uv pip install -qqq --upgrade \\\n        unsloth {get_vllm} {get_numpy} torchvision bitsandbytes xformers\n    !uv pip install -qqq {get_triton}\n!uv pip install transformers==4.53.0\n!uv pip install --no-deps trl==0.22.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:36:47.965316Z","iopub.execute_input":"2025-09-27T09:36:47.965604Z","iopub.status.idle":"2025-09-27T09:36:53.988170Z","shell.execute_reply.started":"2025-09-27T09:36:47.965581Z","shell.execute_reply":"2025-09-27T09:36:53.987460Z"}},"outputs":[{"name":"stdout","text":"\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m30 packages\u001b[0m \u001b[2min 17ms\u001b[0m\u001b[0m                                         \u001b[0m\n\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 84ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 73ms\u001b[0m\u001b[0m                                \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.4\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.56.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.53.0\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m                                            \u001b[0m\n\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m                                  \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.23.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.22.2\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Base model\nbase_model_name = \"unsloth/Qwen3-1.7B-Base\"\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"cuda:0\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\n# Load LoRA adapter locally\nadapter_path = \"/kaggle/input/qwen_grpo_medmcqa/transformers/default/1/grpo-medmcqa-qwen-4b/checkpoint-400\"\nmodel = PeftModel.from_pretrained(model, adapter_path)\n\n# Move model to GPU\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:36:59.652242Z","iopub.execute_input":"2025-09-27T09:36:59.652577Z","iopub.status.idle":"2025-09-27T09:38:13.469329Z","shell.execute_reply.started":"2025-09-27T09:36:59.652543Z","shell.execute_reply":"2025-09-27T09:38:13.468554Z"}},"outputs":[{"name":"stderr","text":"2025-09-27 09:37:13.107365: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758965833.461332     175 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758965833.566598     175 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/752 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ec1dbe81c2b4d7bb616c0fadeb7703c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41e716013b82443d96568ccaf4251ff8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eb2cb7d9bad4856ab2dbe03cfb9f7fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38ecc990c04442e2b525b83940c923f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"614f092e94ad401f955193c7e34f4a75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1801438002964cf0b3e88f4ea729cbd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11a43bcf253749bbb7ae837f1d03c793"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"345de1b9b387405085bcd59f2666e543"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/617 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7177e1b173f14d6bb1af024e16a06844"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen3ForCausalLM(\n      (model): Qwen3Model(\n        (embed_tokens): Embedding(151936, 2048, padding_idx=151654)\n        (layers): ModuleList(\n          (0-27): 28 x Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=6144, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=6144, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=6144, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=6144, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=6144, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=6144, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n          )\n        )\n        (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n        (rotary_emb): Qwen3RotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"!pip -qqq install rouge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:38:26.642817Z","iopub.execute_input":"2025-09-27T09:38:26.643081Z","iopub.status.idle":"2025-09-27T09:38:29.844459Z","shell.execute_reply.started":"2025-09-27T09:38:26.643064Z","shell.execute_reply":"2025-09-27T09:38:29.843290Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from datasets import load_dataset\nimport re\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rouge import Rouge\nfrom vllm import SamplingParams\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T09:38:33.949540Z","iopub.execute_input":"2025-09-27T09:38:33.949870Z","iopub.status.idle":"2025-09-27T09:38:35.899973Z","shell.execute_reply.started":"2025-09-27T09:38:33.949840Z","shell.execute_reply":"2025-09-27T09:38:35.899175Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"explanation_start = \"<explanation>\"\nexplanation_end   = \"</explanation>\"\nsolution_start    = \"<answer>\"\nsolution_end      = \"</answer>\"\n\nsystem_prompt = f\"\"\"The User asks a multiple choice question, and the Assistant answers the question from the given options. Think about the problem, pick the option that answers the question and provide your explanation for choosing that option.\n\nFirstly, provide your answer between {solution_start}{solution_end}.\nThen place your explanation between {explanation_start} and {explanation_end}.\n\nExample format:\n{solution_start}0{solution_end}\n{explanation_start}Your reasoning here{explanation_end}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T10:20:52.449067Z","iopub.execute_input":"2025-09-27T10:20:52.449658Z","iopub.status.idle":"2025-09-27T10:20:52.453810Z","shell.execute_reply.started":"2025-09-27T10:20:52.449633Z","shell.execute_reply":"2025-09-27T10:20:52.453160Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"chat_template = \\\n    \"{% if messages[0]['role'] == 'system' %}\"\\\n        \"{{ messages[0]['content'] + eos_token }}\"\\\n        \"{% set loop_messages = messages[1:] %}\"\\\n    \"{% else %}\"\\\n        \"{{ '{system_prompt}' + eos_token }}\"\\\n        \"{% set loop_messages = messages %}\"\\\n    \"{% endif %}\"\\\n    \"{% for message in loop_messages %}\"\\\n        \"{% if message['role'] == 'user' %}\"\\\n            \"{{ message['content'] }}\"\\\n        \"{% elif message['role'] == 'assistant' %}\"\\\n            \"{{ message['content'] + eos_token }}\"\\\n        \"{% endif %}\"\\\n    \"{% endfor %}\"\\\n    \"{% if add_generation_prompt %}{{ '{reasoning_start}' }}\"\\\n    \"{% endif %}\"\n\n# Replace with out specific template:\nchat_template = chat_template\\\n    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n    .replace(\"'{reasoning_start}'\", f\"'{solution_start}'\")\ntokenizer.chat_template = chat_template","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T10:27:50.521993Z","iopub.execute_input":"2025-09-27T10:27:50.522290Z","iopub.status.idle":"2025-09-27T10:27:50.526797Z","shell.execute_reply.started":"2025-09-27T10:27:50.522267Z","shell.execute_reply":"2025-09-27T10:27:50.526103Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# simple QA test\n#question = \"What is the capital of France?\"\nq = \"The most common organism causing urinary tract infections is:\\nOptions:\\n0. Staphylococcus aureus\\n1. Escherichia coli\\n2. Pseudomonas aeruginosa\\n3. Enterococcus faecalis\"\n\nmessages = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\",   \"content\": q},\n]\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt = True, # Must add for generation\n    tokenize = False,\n)\n\ninputs = tokenizer(text, return_tensors=\"pt\").to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T10:27:56.047241Z","iopub.execute_input":"2025-09-27T10:27:56.048158Z","iopub.status.idle":"2025-09-27T10:27:57.836840Z","shell.execute_reply.started":"2025-09-27T10:27:56.048123Z","shell.execute_reply":"2025-09-27T10:27:57.836094Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"\"The User asks a multiple choice question, and the Assistant answers the question from the given options. Think about the problem, pick the option that answers the question and provide your explanation for choosing that option.\\n\\nFirstly, provide your answer between <answer></answer>.\\nThen place your explanation between <explanation> and </explanation>.\\n\\nExample format:\\n<answer>0</answer>\\n<explanation>Your reasoning here</explanation>The most common organism causing urinary tract infections is:\\nOptions:\\n0. Staphylococcus aureus\\n1. Escherichia coli\\n2. Pseudomonas aeruginosa\\n3. Enterococcus faecalis<answer>1</answer><explanation>Ans. is 'b' i.e., Escherichia coli</explanation>\""},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nFastLanguageModel.for_inference(model)\ninputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\nwith torch.no_grad():\n    output = model.generate(\n        **inputs,\n        max_new_tokens=100,\n        temperature=0.1,\n        do_sample=True\n    )\nresult = tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\nprint(\"Transformers generation result:\")\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T10:23:53.685867Z","iopub.execute_input":"2025-09-27T10:23:53.686523Z","iopub.status.idle":"2025-09-27T10:23:55.474988Z","shell.execute_reply.started":"2025-09-27T10:23:53.686475Z","shell.execute_reply":"2025-09-27T10:23:55.474198Z"}},"outputs":[{"name":"stdout","text":"Transformers generation result:\n1</answer><explanation>Ans. is 'b' i.e., Escherichia coli</explanation>\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"def extract_answer_and_explanation(text):\n    \"\"\"Extract answer and explanation from model output\"\"\"\n    # Look for pattern: number</answer><explanation>text</explanation>\n    answer_match = re.search(r'(\\d+)</answer>', text)\n    explanation_match = re.search(r'<explanation>(.*?)</explanation>', text, re.DOTALL)\n    \n    predicted_answer = int(answer_match.group(1)) if answer_match else None\n    explanation = explanation_match.group(1).strip() if explanation_match else \"\"\n    \n    return predicted_answer, explanation\n\ndef generate_response(model, tokenizer, question, system_prompt, device=\"cuda\"):\n    \"\"\"Generate model response for a question\"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": question},\n    ]\n    \n    text = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        tokenize=False,\n    )\n    \n    FastLanguageModel.for_inference(model)\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    \n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.1,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    result = tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n    return result\n\ndef format_question(example):\n    \"\"\"Format a dataset example into question format\"\"\"\n    question = f\"\"\"{example[\"question\"]}\nOptions:\n0. {example[\"opa\"]}\n1. {example[\"opb\"]}\n2. {example[\"opc\"]}\n3. {example[\"opd\"]}\"\"\"\n    return question\n\ndef evaluate_model(model, tokenizer, system_prompt, num_samples=10):\n    \"\"\"Evaluate model on MedMCQA validation set\"\"\"\n    # Load validation dataset\n    dataset = load_dataset(\"openlifescienceai/medmcqa\", split=\"validation\")\n    \n    # Take first num_samples for evaluation\n    eval_samples = dataset.select(range(num_samples))\n    \n    results = []\n    correct_predictions = 0\n    \n    print(\"Evaluating model...\")\n    print(\"=\" * 80)\n    \n    for i, example in enumerate(eval_samples):\n        question = format_question(example)\n        correct_answer = example[\"cop\"]\n        \n        # Generate response\n        response = generate_response(model, tokenizer, question, system_prompt)\n        \n        # Extract answer and explanation\n        predicted_answer, explanation = extract_answer_and_explanation(response)\n        \n        # Check accuracy\n        is_correct = predicted_answer == correct_answer\n        if is_correct:\n            correct_predictions += 1\n        \n        # Store result\n        result = {\n             'question_id': i,\n             'question': example[\"question\"],\n             'correct_answer': correct_answer,\n             'predicted_answer': predicted_answer,\n             'is_correct': is_correct,\n             'explanation': explanation,\n             'full_response': response\n         }\n        results.append(result)\n        \n        # Print individual result\n        print(f\"Question {i+1}/{num_samples}\")\n        print(f\"Q: {example['question']}\")\n        print(f\"Correct: {correct_answer}, Predicted: {predicted_answer}, {'✓' if is_correct else '✗'}\")\n        print(f\"Response: {response}\")\n        print(\"-\" * 40)\n    \n    # Calculate metrics\n    accuracy = correct_predictions / num_samples\n    \n    # Summary\n    print(\"\\nEVALUATION SUMMARY\")\n    print(\"=\" * 50)\n    print(f\"Total samples: {num_samples}\")\n    print(f\"Correct predictions: {correct_predictions}\")\n    print(f\"Accuracy: {accuracy:.2%}\")\n    \n    # Format validation\n    format_valid_count = sum(1 for r in results if r['predicted_answer'] is not None)\n    format_accuracy = format_valid_count / num_samples\n    print(f\"Format compliance: {format_accuracy:.2%} ({format_valid_count}/{num_samples})\")\n    \n    # Show some examples\n    print(f\"\\nSample predictions:\")\n    for i in range(min(3, num_samples)):\n        r = results[i]\n        print(f\"Q: {r['question'][:60]}...\")\n        print(f\"A: {r['predicted_answer']} ({'Correct' if r['is_correct'] else 'Wrong'})\")\n        print(f\"Explanation: {r['explanation'][:80]}...\")\n        print()\n    \n    return results, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T10:46:19.098850Z","iopub.execute_input":"2025-09-27T10:46:19.099602Z","iopub.status.idle":"2025-09-27T10:46:19.111065Z","shell.execute_reply.started":"2025-09-27T10:46:19.099513Z","shell.execute_reply":"2025-09-27T10:46:19.110301Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"results, accuracy = evaluate_model(model, tokenizer, system_prompt, num_samples=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T10:46:23.870014Z","iopub.execute_input":"2025-09-27T10:46:23.870814Z","iopub.status.idle":"2025-09-27T10:46:53.790321Z","shell.execute_reply.started":"2025-09-27T10:46:23.870789Z","shell.execute_reply":"2025-09-27T10:46:53.789572Z"}},"outputs":[{"name":"stdout","text":"Evaluating model...\n================================================================================\nQuestion 1/10\nQ: Which of the following is not true for myelinated nerve fibers:\nCorrect: 0, Predicted: 0, ✓\nResponse: 0</answer><explanation>Ans. is 'a' i.e., Impulse through myelinated fibers is slower than non-myelinated fibers</explanation>\n----------------------------------------\nQuestion 2/10\nQ: Which of the following is not true about glomerular capillaries')\nCorrect: 0, Predicted: 3, ✗\nResponse: 3</answer><explanation>Glomerular capillaries are continuous with the afferent and efferent arterioles. The blood flow through the glomerular capillaries is constant. The blood flow is regulated by constriction or dilation of the afferent arteriole. The oncotic pressure of the fluid leaving the capillaries is greater than that of fluid entering it. The concentration of glucose in the capillaries is greater than that in the glomerular filtrate. The hematocrit of the fluid leaving the capillaries is greater than that of the fluid entering it.</explanation>\n----------------------------------------\nQuestion 3/10\nQ: A 29 yrs old woman with a pregnancy of 17 week has a 10 years old boy with down syndrome. She does not want another down syndrome kid; best advice to her is\nCorrect: 2, Predicted: 2, ✓\nResponse: 2</answer><explanation>Ans. is 'c' i.e., Amniotic fluid samples plus chromosomal analysis will definitely tell her that next baby will be down syndromic or not</explanation>\n----------------------------------------\nQuestion 4/10\nQ: Axonal transport is:\nCorrect: 2, Predicted: 2, ✓\nResponse: 2</answer><explanation>Ans. is 'c' i.e., Antegrade and retrograde</explanation>\n----------------------------------------\nQuestion 5/10\nQ: Low insulin to glucagon ratio is seen in all of these except:\nCorrect: 0, Predicted: 1, ✗\nResponse: 1</answer><explanation>Ans. is 'b' i.e., Glycogen breakdown</explanation>\n----------------------------------------\nQuestion 6/10\nQ: Concentration of tropicamide:\nCorrect: 0, Predicted: 1, ✗\nResponse: 1</answer><explanation>Ans. is 'b' i.e., 0.02</explanation>\n----------------------------------------\nQuestion 7/10\nQ: Which of the following statements is true regarding H I N1 Influenza?\nCorrect: 0, Predicted: 0, ✓\nResponse: 0</answer><explanation>Ans. is 'a' i.e., Pregnant woman with sore throat can be staed immediately on oseltamivir without diagnostic testing under category B</explanation>\n----------------------------------------\nQuestion 8/10\nQ: Which of the following are not a branch of external carotid Aery in Kiesselbach's plexus.\nCorrect: 1, Predicted: 3, ✗\nResponse: 3</answer><explanation>Ans. is 'd' i.e., Septal branch of superior labial aery</explanation>\n----------------------------------------\nQuestion 9/10\nQ: Diagnosis of the following ECG-\nCorrect: 1, Predicted: 0, ✗\nResponse: 0</answer><explanation>Ans. is 'a' i.e., Ventricular bigeminy</explanation>\n----------------------------------------\nQuestion 10/10\nQ: A blue new born presents with cyanosis. The X–ray chest reveal oligaemic lung field and normal sized heart. Most likely diagnosis is –\nCorrect: 1, Predicted: 3, ✗\nResponse: 3</answer><explanation>Tetralogy of fallot is the most common cyanotic congenital heart disease. It is characterized by pulmonary stenosis, ventricular septal defect, right ventricular hypertrophy and overriding of the aorta. Ref: Harrison's Principles of Internal Medicine, 18th Edition, Page 1335.</explanation>\n----------------------------------------\n\nEVALUATION SUMMARY\n==================================================\nTotal samples: 10\nCorrect predictions: 4\nAccuracy: 40.00%\nFormat compliance: 100.00% (10/10)\n\nSample predictions:\nQ: Which of the following is not true for myelinated nerve fibe...\nA: 0 (Correct)\nExplanation: Ans. is 'a' i.e., Impulse through myelinated fibers is slower than non-myelinate...\n\nQ: Which of the following is not true about glomerular capillar...\nA: 3 (Wrong)\nExplanation: Glomerular capillaries are continuous with the afferent and efferent arterioles....\n\nQ: A 29 yrs old woman with a pregnancy of 17 week has a 10 year...\nA: 2 (Correct)\nExplanation: Ans. is 'c' i.e., Amniotic fluid samples plus chromosomal analysis will definite...\n\n","output_type":"stream"}],"execution_count":61}]}